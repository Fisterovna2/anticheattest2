import time
import random
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from collections import deque

class GameState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã"""
    def __init__(self):
        self.hero_health = 100.0
        self.hero_mana = 100.0
        self.hero_level = 1
        self.hero_gold = 0
        self.position = (0, 0)
        self.nearby_allies = 0
        self.nearby_enemies = 0
        self.game_time = 0
        self.networth_advantage = 0

class HeroRole(Enum):
    CARRY = "carry"
    MID = "mid"
    OFFLANE = "offlane"
    SUPPORT = "support"

class LearningAIBrain:
    """
    AI –° –û–ë–£–ß–ï–ù–ò–ï–ú - —É—á–∏—Ç—Å—è –Ω–∞ –æ–ø—ã—Ç–µ –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è
    """
    
    def __init__(self, memory_manager, metrics_collector, role: HeroRole = HeroRole.CARRY):
        self.memory = memory_manager
        self.metrics = metrics_collector
        self.role = role
        
        # –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (–±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é)
        self.rl_learner = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑–≤–Ω–µ
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.last_state = None
        self.last_action = None
        self.last_state_vector = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.learning_enabled = True
        self.total_learning_rewards = 0
        self.successful_actions = 0
        self.failed_actions = 0
        self.decision_history = deque(maxlen=1000)
        self.last_decision_time = 0
        
        print(f"üß† Learning AI Brain –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Ä–æ–ª—å: {role.value})")
    
    def analyze_game_state(self) -> GameState:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä—ã"""
        game_state = GameState()
        
        try:
            # –≠–º—É–ª—è—Ü–∏—è —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–≥—Ä—ã
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞ —Å memory_manager
            
            # –°–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            game_state.hero_health = random.uniform(0.1, 1.0)
            game_state.hero_mana = random.uniform(0.1, 1.0)
            game_state.hero_level = random.randint(1, 25)
            game_state.hero_gold = random.randint(0, 10000)
            game_state.position = (random.uniform(0, 15000), random.uniform(0, 15000))
            game_state.nearby_allies = random.randint(0, 4)
            game_state.nearby_enemies = random.randint(0, 5)
            game_state.game_time = time.time() % 3600
            game_state.networth_advantage = random.uniform(-5000, 5000)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä—ã: {e}")
        
        return game_state
    
    def decide_next_action(self, game_state: GameState) -> Dict:
        """–ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π AI"""
        try:
            current_time = time.time()
            if current_time - self.last_decision_time < 1.0:
                return {"action": "wait", "reason": "cooldown"}
            
            # –ê–Ω–∞–ª–∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            environment = {
                'threat_level': random.uniform(0, 1),
                'farm_opportunity': random.uniform(0, 1),
                'enemy_heroes': game_state.nearby_enemies
            }
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            if self.rl_learner:
                state_vector = self.rl_learner.get_state_vector(game_state)
                
                # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–º–æ—â—å—é RL
                available_actions = ["attack", "retreat", "farm", "objective"]
                action_index = self.rl_learner.choose_action(state_vector, available_actions)
                rl_action = available_actions[action_index]
            else:
                # –ó–∞–≥–ª—É—à–∫–∞ –µ—Å–ª–∏ RL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
                rl_action = random.choice(["attack", "retreat", "farm", "objective"])
                state_vector = np.zeros(8)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            if self.last_state_vector is not None and self.learning_enabled and self.rl_learner:
                # –í—ã—á–∏—Å–ª—è–µ–º –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
                reward = self._compute_learning_reward(game_state, environment)
                self.rl_learner.remember(
                    self.last_state_vector, 
                    self.last_action, 
                    reward, 
                    state_vector, 
                    False
                )
                
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–ø—ã—Ç–µ
                self.rl_learner.learn()
                self.total_learning_rewards += reward
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self.last_state = game_state
            self.last_action = action_index if self.rl_learner else 0
            self.last_state_vector = state_vector
            
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ç–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è
            final_action = self._apply_tactical_rules(rl_action, game_state, environment)
            
            # –ó–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é
            decision_record = {
                "timestamp": current_time,
                "action": final_action,
                "rl_action": rl_action,
                "environment": environment,
                "confidence": 1.0 - (self.rl_learner.epsilon if self.rl_learner else 0.5),
                "learning_reward": self.total_learning_rewards
            }
            self.decision_history.append(decision_record)
            self.last_decision_time = current_time
            
            return {
                "action": final_action,
                "rl_action": rl_action,
                "reason": self._get_learning_action_reason(final_action, rl_action, environment),
                "confidence": (1.0 - (self.rl_learner.epsilon if self.rl_learner else 0.5)) * 100,
                "environment_analysis": environment,
                "learning_enabled": self.learning_enabled
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è —Å –æ–±—É—á–µ–Ω–∏–µ–º: {e}")
            return {"action": "wait", "reason": "error", "confidence": 0}
    
    def _compute_learning_reward(self, current_state: GameState, environment: Dict) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π"""
        if self.last_state is None or self.last_action is None:
            return 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        health_improvement = current_state.hero_health - self.last_state.hero_health
        gold_improvement = current_state.hero_gold - self.last_state.hero_gold
        level_improvement = current_state.hero_level - self.last_state.hero_level
        
        reward = 0.0
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ
        if current_state.hero_health > 0.1:  # –í—ã–∂–∏–ª–∏
            reward += 5.0
        else:
            reward -= 20.0  # –°–º–µ—Ä—Ç—å - –±–æ–ª—å—à–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å
        reward += health_improvement * 10.0
        reward += gold_improvement / 10.0
        reward += level_improvement * 5.0
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É—Å–ø–µ—Ö–∏
        if environment['threat_level'] < 0.3:  # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∞
            reward += 3.0
        elif environment['threat_level'] > 0.7:  # –û–ø–∞—Å–Ω–∞—è –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∞
            reward -= 5.0
        
        if environment['farm_opportunity'] > 0.5:  # –•–æ—Ä–æ—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∞—Ä–º–∞
            reward += 2.0
        
        return reward
    
    def _apply_tactical_rules(self, rl_action: str, game_state: GameState, environment: Dict) -> str:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª –∫ —Ä–µ—à–µ–Ω–∏—é RL"""
        # –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —Ä–µ—à–µ–Ω–∏—è RL
        if game_state.hero_health < 0.2:
            return "retreat"  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º HP
        
        if environment['threat_level'] > 0.8 and game_state.nearby_allies < 2:
            return "retreat"  # –û—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ –ø—Ä–∏ —á–∏—Å–ª–µ–Ω–Ω–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–µ –≤—Ä–∞–≥–∞
        
        if rl_action == "attack" and environment['enemy_heroes'] == 0:
            return "farm"  # –ù–µ—Ç –≤—Ä–∞–≥–æ–≤ –¥–ª—è –∞—Ç–∞–∫–∏ - —Ñ–∞—Ä–º–∏–º
        
        if rl_action == "farm" and environment['farm_opportunity'] < 0.2:
            return "objective"  # –ù–µ—Ç –∫—Ä–∏–ø–æ–≤ - –∏–¥–µ–º –∫ —Ü–µ–ª–∏
        
        return rl_action  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ—à–µ–Ω–∏–µ RL
    
    def _get_learning_action_reason(self, final_action: str, rl_action: str, environment: Dict) -> str:
        """–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è"""
        base_reasons = {
            "attack": "–ê—Ç–∞–∫—É—é –≤—ã–±—Ä–∞–Ω–Ω—É—é —Ü–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
            "retreat": "–û—Ç—Å—Ç—É–ø–∞—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–µ—Ä–æ—è –∏ –ø–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏", 
            "farm": "–§–∞—Ä–º–ª—é —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≥–µ—Ä–æ—è",
            "objective": "–ó–∞—Ö–≤–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏",
            "wait": "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–∏—Ç—É–∞—Ü–∏—é –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –¥–µ–π—Å—Ç–≤–∏–µ–º"
        }
        
        reason = base_reasons.get(final_action, "–ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Ç—É–∞—Ü–∏–∏")
        
        if final_action != rl_action:
            reason += f" (RL –ø—Ä–µ–¥–ª–æ–∂–∏–ª {rl_action}, —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏)"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        if environment['threat_level'] > 0.6:
            reason += " - –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —É–≥—Ä–æ–∑—ã"
        elif environment['farm_opportunity'] > 0.7:
            reason += " - –ë–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ñ–∞—Ä–º–∞"
        
        return reason
    
    def get_learning_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        total_actions = self.successful_actions + self.failed_actions
        
        base_stats = {
            'total_decisions': len(self.decision_history),
            'successful_actions': self.successful_actions,
            'failed_actions': self.failed_actions,
            'success_rate': self.successful_actions / total_actions if total_actions > 0 else 0,
            'learning_enabled': self.learning_enabled,
            'total_learning_rewards': self.total_learning_rewards
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º RL —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.rl_learner:
            rl_stats = self.rl_learner.get_learning_stats()
            base_stats.update(rl_stats)
        
        return base_stats
    
    def save_learning_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if self.rl_learner:
            self.rl_learner.save_model()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
        learning_data = {
            'decision_history': list(self.decision_history),
            'successful_actions': self.successful_actions,
            'failed_actions': self.failed_actions,
            'total_learning_rewards': self.total_learning_rewards,
            'save_timestamp': time.time()
        }
        
        import json
        try:
            with open('models/learning_progress.json', 'w') as f:
                json.dump(learning_data, f, indent=2)
            print("üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
    
    def enable_learning(self, enabled: bool = True):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è"""
        self.learning_enabled = enabled
        status = "–≤–∫–ª—é—á–µ–Ω–æ" if enabled else "–≤—ã–∫–ª—é—á–µ–Ω–æ"
        print(f"üéØ –û–±—É—á–µ–Ω–∏–µ AI {status}")