import numpy as np
import random
import json
import os
from collections import deque
import heapq
from typing import Dict, List, Tuple, Optional
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import time

class PrioritizedReplayBuffer:
    """–ë—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –æ–ø—ã—Ç–æ–≤"""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6, beta: float = 0.4):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        self.size = 0
        
    def add(self, state, action, reward, next_state, done):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º"""
        max_priority = self.priorities.max() if self.size > 0 else 1.0
        
        if self.size < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
            self.size += 1
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done)
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> Tuple:
        """–í—ã–±–æ—Ä–∫–∞ –±–∞—Ç—á–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
        if self.size == 0:
            return None
            
        priorities = self.priorities[:self.size]
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        
        indices = np.random.choice(self.size, batch_size, p=probabilities)
        weights = (self.size * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()
        
        batch = [self.buffer[idx] for idx in indices]
        states, actions, rewards, next_states, dones = zip(*batch)
        
        return (
            torch.FloatTensor(states),
            torch.LongTensor(actions),
            torch.FloatTensor(rewards),
            torch.FloatTensor(next_states),
            torch.BoolTensor(dones),
            torch.FloatTensor(weights),
            indices
        )
    
    def update_priorities(self, indices: List[int], priorities: List[float]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –æ–ø—ã—Ç–æ–≤"""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority + 1e-5  # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–ª–µ–Ω—å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω—É–ª—è

class NoisyLinear(nn.Module):
    """–®—É–º–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, in_features: int, out_features: int, sigma_init: float = 0.5):
        super(NoisyLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.sigma_init = sigma_init
        
        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.Tensor(out_features))
        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))
        
        self.register_buffer('weight_epsilon', torch.Tensor(out_features, in_features))
        self.register_buffer('bias_epsilon', torch.Tensor(out_features))
        
        self.reset_parameters()
        self.reset_noise()
    
    def reset_parameters(self):
        """–°–±—Ä–æ—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        mu_range = 1 / np.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.sigma_init / np.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.sigma_init / np.sqrt(self.out_features))
    
    def reset_noise(self):
        """–°–±—Ä–æ—Å —à—É–º–∞"""
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)
        
        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)
    
    def _scale_noise(self, size: int) -> torch.Tensor:
        """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —à—É–º–∞"""
        x = torch.randn(size)
        return x.sign().mul(x.abs().sqrt())
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å —à—É–º–æ–º"""
        if self.training:
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        
        return F.linear(x, weight, bias)

class AdvancedDQN(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è DQN —Å–µ—Ç—å —Å —à—É–º–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128, noisy: bool = True):
        super(AdvancedDQN, self).__init__()
        
        LinearLayer = NoisyLinear if noisy else nn.Linear
        
        self.fc1 = LinearLayer(state_size, hidden_size)
        self.fc2 = LinearLayer(hidden_size, hidden_size)
        self.fc3 = LinearLayer(hidden_size, hidden_size // 2)
        
        # Dueling DQN
        self.value_stream = LinearLayer(hidden_size // 2, 1)
        self.advantage_stream = LinearLayer(hidden_size // 2, action_size)
        
        self.noisy = noisy
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å dueling architecture"""
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        
        value = self.value_stream(x)
        advantages = self.advantage_stream(x)
        
        # Dueling DQN: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))
        
        return q_values
    
    def reset_noise(self):
        """–°–±—Ä–æ—Å —à—É–º–∞ –≤–æ –≤—Å–µ—Ö —Å–ª–æ—è—Ö"""
        if self.noisy:
            for module in [self.fc1, self.fc2, self.fc3, self.value_stream, self.advantage_stream]:
                if hasattr(module, 'reset_noise'):
                    module.reset_noise()

class AdvancedReinforcementLearner:
    """
    –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø –° –ü–û–î–ö–†–ï–ü–õ–ï–ù–ò–ï–ú
    –° —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, state_size: int = 8, action_size: int = 4, learning_rate: float = 0.0001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
        self.policy_net = AdvancedDQN(state_size, action_size, noisy=True)
        self.target_net = AdvancedDQN(state_size, action_size, noisy=False)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
        self.memory = PrioritizedReplayBuffer(capacity=20000)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.batch_size = 64
        self.gamma = 0.99  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.tau = 0.005  # –î–ª—è –º—è–≥–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è target —Å–µ—Ç–∏
        self.learn_step_counter = 0
        self.update_target_every = 100
        
        # –¢—Ä–µ–∫–µ—Ä—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        self.reward_history = []
        self.loss_history = []
        self.q_value_history = []
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.load_model()
        
        print("üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RL —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print(f"   - Dueling DQN —Å Noisy Layers")
        print(f"   - Prioritized Experience Replay")
        print(f"   - Double DQN + Soft Updates")
    
    def get_state_vector(self, game_state) -> np.ndarray:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä—ã –≤ –≤–µ–∫—Ç–æ—Ä –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        return np.array([
            game_state.hero_health,           # 0-1
            game_state.hero_mana,             # 0-1  
            game_state.hero_level / 25.0,     # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
            min(game_state.hero_gold / 10000.0, 1.0),  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–æ–ª–æ—Ç–æ
            game_state.nearby_enemies / 5.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—Ä–∞–≥–∏
            game_state.nearby_allies / 4.0,   # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–æ—é–∑–Ω–∏–∫–∏
            game_state.game_time / 3600.0,    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            game_state.networth_advantage / 10000.0  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
        ], dtype=np.float32)
    
    def choose_action(self, state_vector: np.ndarray, available_actions: List[str]) -> int:
        """–í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º noisy DQN (–±–µ–∑ epsilon-greedy)"""
        state_tensor = torch.FloatTensor(state_vector).unsqueeze(0)
        
        with torch.no_grad():
            q_values = self.policy_net(state_tensor)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º Q –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.q_value_history.append(q_values.max().item())
        if len(self.q_value_history) > 1000:
            self.q_value_history.pop(0)
        
        return np.argmax(q_values.numpy())
    
    def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä"""
        self.memory.add(state, action, reward, next_state, done)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–≥—Ä–∞–¥—É –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        self.reward_history.append(reward)
        if len(self.reward_history) > 1000:
            self.reward_history.pop(0)
    
    def compute_advanced_reward(self, game_state, action: str, success: bool, 
                              environment: Dict, previous_state = None) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã"""
        reward = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ –¥–µ–π—Å—Ç–≤–∏—è
        action_rewards = {
            "attack": 8.0 if success and environment.get('enemy_heroes', 0) > 0 else -3.0,
            "retreat": 12.0 if success and game_state.hero_health < 0.3 else -2.0,
            "farm": 6.0 if success and environment.get('farm_opportunity', 0) > 0.3 else -1.0,
            "objective": 15.0 if success else -2.0,
            "wait": -0.5
        }
        reward += action_rewards.get(action, 0.0)
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ (–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ)
        if game_state.hero_health > 0.9:
            reward += 5.0
        elif game_state.hero_health < 0.1:
            reward -= 20.0  # –ë–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ —Å–º–µ—Ä—Ç—å
        elif game_state.hero_health < 0.3:
            reward -= 5.0   # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫–æ–µ HP
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å
        if previous_state:
            # –£–ª—É—á—à–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            health_improvement = game_state.hero_health - previous_state.hero_health
            gold_improvement = game_state.hero_gold - previous_state.hero_gold
            level_improvement = game_state.hero_level - previous_state.hero_level
            
            reward += health_improvement * 20.0
            reward += gold_improvement / 50.0
            reward += level_improvement * 10.0
        
        # –ö–æ–º–∞–Ω–¥–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã
        if environment.get('threat_level', 0) < 0.2:
            reward += 3.0  # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
        if environment.get('farm_opportunity', 0) > 0.7:
            reward += 4.0  # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ö–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è —Ñ–∞—Ä–º–∞
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        reward += min(game_state.hero_gold / 1000.0, 10.0)  # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –∑–æ–ª–æ—Ç–æ, –Ω–æ –Ω–µ –±–æ–ª–µ–µ 10
        reward += game_state.hero_level * 2.0  # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —É—Ä–æ–≤–Ω–∏
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ –≤ —Ö–æ—Ä–æ—à–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö
        if action == "wait" and environment.get('farm_opportunity', 0) > 0.5:
            reward -= 3.0
        
        return reward
    
    def learn(self):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å Double DQN –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ–º"""
        if self.memory.size < self.batch_size:
            return
        
        # –í—ã–±–æ—Ä–∫–∞ –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±—É—Ñ–µ—Ä–∞
        sample = self.memory.sample(self.batch_size)
        if sample is None:
            return
            
        states, actions, rewards, next_states, dones, weights, indices = sample
        
        # Double DQN
        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)
        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze()
        expected_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # –¢–µ–∫—É—â–∏–µ Q –∑–Ω–∞—á–µ–Ω–∏—è
        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å —Å –≤–µ—Å–∞–º–∏
        td_errors = (expected_q_values - current_q_values).abs().detach().numpy()
        loss = (weights * F.mse_loss(current_q_values, expected_q_values, reduction='none')).mean()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        self.memory.update_priorities(indices, td_errors)
        
        # –ú—è–≥–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ target —Å–µ—Ç–∏
        self.soft_update_target_network()
        
        # –°–±—Ä–æ—Å —à—É–º–∞
        self.policy_net.reset_noise()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.loss_history.append(loss.item())
        if len(self.loss_history) > 1000:
            self.loss_history.pop(0)
        
        self.learn_step_counter += 1
    
    def soft_update_target_network(self):
        """–ú—è–≥–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ target —Å–µ—Ç–∏"""
        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(self.tau * policy_param.data + (1.0 - self.tau) * target_param.data)
    
    def save_model(self, filepath: str = "models/advanced_rl_model.pth"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'reward_history': self.reward_history,
            'loss_history': self.loss_history,
            'learn_step_counter': self.learn_step_counter
        }, filepath)
        print(f"üíæ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RL –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
    
    def load_model(self, filepath: str = "models/advanced_rl_model.pth"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if os.path.exists(filepath):
            checkpoint = torch.load(filepath)
            self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
            self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.reward_history = checkpoint.get('reward_history', [])
            self.loss_history = checkpoint.get('loss_history', [])
            self.learn_step_counter = checkpoint.get('learn_step_counter', 0)
            print(f"üíæ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
    
    def plot_learning_progress(self, save_path: str = "visualizations/learning_progress.png"):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if len(self.reward_history) < 10:
            return
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥
        ax1.plot(self.reward_history)
        ax1.set_title('–ò—Å—Ç–æ—Ä–∏—è –Ω–∞–≥—Ä–∞–¥')
        ax1.set_xlabel('–®–∞–≥')
        ax1.set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
        ax1.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        if self.loss_history:
            ax2.plot(self.loss_history)
            ax2.set_title('–ò—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å')
            ax2.set_xlabel('–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è')
            ax2.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
            ax2.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ Q –∑–Ω–∞—á–µ–Ω–∏–π
        if self.q_value_history:
            ax3.plot(self.q_value_history)
            ax3.set_title('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ Q –∑–Ω–∞—á–µ–Ω–∏—è')
            ax3.set_xlabel('–®–∞–≥')
            ax3.set_ylabel('Q –∑–Ω–∞—á–µ–Ω–∏–µ')
            ax3.grid(True)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_text = f"""
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:
        –®–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {self.learn_step_counter}
        –†–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏: {self.memory.size}
        –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(self.reward_history[-100:]):.2f}
        –°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {np.mean(self.loss_history[-100:]):.2f}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10, verticalalignment='top')
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
    
    def get_learning_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        recent_rewards = self.reward_history[-100:] if self.reward_history else [0]
        recent_losses = self.loss_history[-100:] if self.loss_history else [0]
        
        return {
            'learn_steps': self.learn_step_counter,
            'memory_size': self.memory.size,
            'avg_reward': np.mean(recent_rewards),
            'avg_loss': np.mean(recent_losses),
            'std_reward': np.std(recent_rewards),
            'max_reward': np.max(recent_rewards) if recent_rewards else 0,
            'min_reward': np.min(recent_rewards) if recent_rewards else 0,
            'avg_q_value': np.mean(self.q_value_history[-100:]) if self.q_value_history else 0
        }